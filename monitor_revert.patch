diff --git a/.gitignore b/.gitignore
index e522ec7..b362730 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,67 +1,24 @@
-# Python
-__pycache__/
-.pytest_cache/
-.mypy_cache/
-.ruff_cache/
-.coverage
-coverage.xml
-.venv/
-uv.lock
-.DS_Store
-*.pyc
-*.pyo
-*.pyd
-*.egg-info/
-dist/
-build/
-
-# Authentication (NEVER commit!)
-.mcp-auth/
-tokens.json
-*_tokens.json
-
-# Local development
-.archive_cleanup/
-.archive_old/
-archive/
-test_*.py
-send_test*.py
-simple_*.py
-mcp_client_simple.py
-
-# Local configs (keep examples)
-configs/mcp_config.json
-configs/mcp_config_*.json
-!configs/mcp_config.example.json
-
-# Local development configs
-configs/feature_flags.example.sh
-configs/thinking_tags_config.json
-prompts/ax_base_system_prompt.txt
-prompts/ax_code_support_overlay.txt
+# Log and runtime artifacts
+logs/
+*.log
 
-# Reliable monitor data
-data/
-messages*.db
-*.db
+# Python cache
+__pycache__/
+**/__pycache__/
+*.py[cod]
 
-# Evaluation logs
-logs/
+# Virtual environments
+.venv/
 
-# IDE
+# Local tools / editors
 .vscode/
+.DS_Store
 .idea/
-*.swp
-*.swo
 
-# Local docs/notes
-WORKING_SOLUTION.md
-TODO.md
-NOTES.md
-.gemini/
-.mcp.json
-AGENTS.md
-GEMINI.md
-configs/mcp_config_prod.json
-.env
-.pycache/
+# Generated copies / backups
+*.orig
+*.bak
+
+# Requirements summary drafts
+prompt_memo.txt
+
diff --git a/plugins/langgraph_plugin.py b/plugins/langgraph_plugin.py
index edc99c0..98cf231 100644
--- a/plugins/langgraph_plugin.py
+++ b/plugins/langgraph_plugin.py
@@ -129,20 +129,46 @@ class LanggraphPlugin(BasePlugin):
         self._tool_runs = 0
         self._tool_specs: List[Dict[str, Any]] = []
         self._available_tool_names: set[str] = set()
+        streaming_setting = str(
+            self.config.get("streaming", os.getenv("LANGGRAPH_STREAMING", "true"))
+        ).lower()
+        self.enable_streaming = streaming_setting not in {"0", "false", "no"}
+        force_prefix_setting = str(
+            self.config.get("force_sender_prefix", os.getenv("LANGGRAPH_FORCE_MENTION", "false"))
+        ).lower()
+        self.force_sender_prefix = force_prefix_setting in {"1", "true", "yes"}
 
     def on_monitor_context_ready(self) -> None:
         if not self.current_date:
             return
+
         first = self._history[0] if self._history else None
         if first and first.get("role") == "system":
             content = first.get("content", "")
             if f"Current date:" not in content:
-                self._history[0] = {
-                    "role": "system",
-                    "content": f"Current date: {self.current_date}\n\n{content}",
-                }
+                content = f"Current date: {self.current_date}\n\n{content}"
+            allowed_dirs = self.monitor_context.get("allowed_directories") or []
+            if allowed_dirs:
+                dir_list = "\n".join(f"- {path}" for path in allowed_dirs)
+                constraints = (
+                    "Filesystem access is restricted to the directories listed below. "
+                    "When calling filesystem tools (e.g., read_text_file, write_file), you MUST provide a valid 'path' inside these directories, "
+                    "and include required arguments such as 'content' for write_file.\n" + dir_list
+                )
+                if constraints not in content:
+                    content = f"{content}\n\n{constraints}"
+            self._history[0] = {"role": "system", "content": content}
         else:
-            self._history.insert(0, {"role": "system", "content": f"Current date: {self.current_date}"})
+            base_content = f"Current date: {self.current_date}"
+            allowed_dirs = self.monitor_context.get("allowed_directories") or []
+            if allowed_dirs:
+                dir_list = "\n".join(f"- {path}" for path in allowed_dirs)
+                base_content += (
+                    "\n\nFilesystem access is restricted to the directories listed below. "
+                    "When calling filesystem tools (e.g., read_text_file, write_file), include the required arguments and stay within these roots.\n"
+                    f"{dir_list}"
+                )
+            self._history.insert(0, {"role": "system", "content": base_content})
 
     def on_tool_manager_ready(self) -> None:
         self._graph = None  # Rebuild so tools are considered
@@ -285,14 +311,23 @@ class LanggraphPlugin(BasePlugin):
                 required_mentions.append(normalized_sender)
 
         missing_mentions = [
-            handle for handle in required_mentions if not _contains_handle(reply, handle)
+            handle
+            for handle in required_mentions
+            if handle
+            and handle.lower() != "@unknown"
+            and not _contains_handle(reply, handle)
         ]
 
         if missing_mentions:
-            mention_prefix = " ".join(missing_mentions)
-            reply = f"{mention_prefix} — {reply.lstrip('-–—: ')}"
-
-        if self.auto_mention or normalized_sender:
+            primary = missing_mentions[0]
+            reply = reply.lstrip("-–—: ").strip()
+            if not _contains_handle(reply, primary):
+                reply = f"{primary} {reply}" if reply else primary
+            for extra in missing_mentions[1:]:
+                if not _contains_handle(reply, extra):
+                    reply = f"{reply} {extra}".strip()
+
+        if self.force_sender_prefix:
             reply = _ensure_sender_prefix(reply, normalized_sender)
 
         self._history = _trim_history(messages_after, self.max_history)
@@ -343,14 +378,22 @@ class LanggraphPlugin(BasePlugin):
         if tool_spec:
             kwargs["tools"] = tool_spec
             kwargs["tool_choice"] = "auto"
-        response = await asyncio.to_thread(self.client.chat.completions.create, **kwargs)
-        choice = response.choices[0]
-        message = choice.message
-        message_dict = message.model_dump(mode="json")
+        use_streaming = self.enable_streaming and not tool_spec
+        if use_streaming:
+            stream_result = await asyncio.to_thread(self._stream_completion, kwargs)
+            if stream_result is None:
+                use_streaming = False
+            else:
+                message_dict, pending_calls = stream_result
+        if not use_streaming:
+            response = await asyncio.to_thread(self.client.chat.completions.create, **kwargs)
+            choice = response.choices[0]
+            message = choice.message
+            message_dict = message.model_dump(mode="json")
+            pending_calls = []
         content = message_dict.get("content")
         if isinstance(content, list):
             message_dict["content"] = "".join(_extract_text_chunks(content))
-        pending_calls = []
         tool_calls = message_dict.get("tool_calls") or []
         if tool_calls:
             pending_calls = list(tool_calls)
@@ -367,6 +410,36 @@ class LanggraphPlugin(BasePlugin):
             next_state["tool_results"] = state.get("tool_results")
         return next_state
 
+    def _stream_completion(self, kwargs: Dict[str, Any]) -> Optional[tuple[Dict[str, Any], list]]:
+        aggregated = ""
+        try:
+            print("📝 Streaming reply:", flush=True)
+            stream = self.client.chat.completions.create(stream=True, **kwargs)
+            for chunk in stream:
+                if not chunk.choices:
+                    continue
+                delta = chunk.choices[0].delta
+                if delta is None:
+                    continue
+                if getattr(delta, "tool_calls", None):
+                    # Tool calls via streaming require more plumbing; fall back.
+                    return None
+                text_delta = getattr(delta, "content", None)
+                if text_delta:
+                    aggregated += text_delta
+                    print(text_delta, end="", flush=True)
+            if aggregated:
+                print("", flush=True)
+        except Exception as exc:  # noqa: BLE001
+            print(f"⚠️ Streaming failed ({exc}); falling back to standard completion", flush=True)
+            return None
+
+        message_dict: Dict[str, Any] = {
+            "role": "assistant",
+            "content": aggregated,
+        }
+        return message_dict, []
+
     async def _node_invoke_tool(self, state: Dict[str, Any]) -> Dict[str, Any]:
         pending_calls = state.get("pending_tool_calls") or []
         messages = state.get("messages", [])
diff --git a/plugins/openrouter_plugin.py b/plugins/openrouter_plugin.py
index b04fac1..fdae5ba 100644
--- a/plugins/openrouter_plugin.py
+++ b/plugins/openrouter_plugin.py
@@ -234,6 +234,24 @@ class OpenrouterPlugin(BasePlugin):
 
         return reply
 
+    def on_monitor_context_ready(self) -> None:
+        allowed_dirs = self.monitor_context.get("allowed_directories") or []
+        if not allowed_dirs or not self.messages_history:
+            return
+
+        note = (
+            "Filesystem access is limited to the directories below. "
+            "When using filesystem tools (e.g., read_text_file, write_file), always supply a 'path' within these directories and include required arguments such as 'content' for write_file.\n"
+            + "\n".join(f"- {path}" for path in allowed_dirs)
+        )
+
+        if self.messages_history[0].get("role") == "system":
+            content = self.messages_history[0].get("content", "")
+            if note not in content:
+                self.messages_history[0]["content"] = f"{content}\n\n{note}".strip()
+        else:
+            self.messages_history.insert(0, {"role": "system", "content": note})
+
     def reset_context(self) -> None:
         if self.messages_history:
             self.messages_history = self.messages_history[:1]
diff --git a/pyproject.toml b/pyproject.toml
index add190f..436a588 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -3,7 +3,7 @@ name = "ax-mcp-wait-client"
 version = "0.1.0"
 description = "aX MCP wait-and-echo client using OAuth and Streamable HTTP"
 readme = "README.md"
-requires-python = ">=3.10"
+requires-python = ">=3.11"
 license = { text = "MIT" }
 authors = [
   { name = "aX Platform", email = "devnull@example.com" }
@@ -13,6 +13,7 @@ dependencies = [
   "openai>=1.106.1",
   "langgraph>=0.6.7",
   "langchain-mcp-adapters>=0.1.10",
+  "mcp-use>=0.1.1",
 ]
 
 [project.scripts]
diff --git a/reliable_monitor.py b/reliable_monitor.py
index 7f50d18..c3fc847 100644
--- a/reliable_monitor.py
+++ b/reliable_monitor.py
@@ -21,7 +21,7 @@ import sqlite3
 import hashlib
 import random
 import re
-from datetime import datetime, timedelta
+from datetime import datetime, timedelta, timezone
 from typing import Optional, Dict, Any, List, Tuple
 from pathlib import Path
 from dataclasses import dataclass, asdict
@@ -31,8 +31,13 @@ from enum import Enum
 sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
 sys.path.insert(0, 'src')
 
-from ax_mcp_wait_client.config_loader import parse_mcp_config, get_default_config_path
+from ax_mcp_wait_client.config_loader import (
+    parse_all_mcp_servers,
+    get_default_config_path,
+)
 from ax_mcp_wait_client.mcp_client import MCPClient
+from ax_mcp_wait_client.mcp_patches import patch_mcp_library
+from mcp_tool_manager import MCPToolManager
 
 class MessageStatus(Enum):
     PENDING = "pending"
@@ -136,7 +141,7 @@ class ReliableMessageStore:
         conn = sqlite3.connect(self.db_path)
         try:
             processed_at = datetime.now() if status in [MessageStatus.COMPLETED, MessageStatus.DEAD_LETTER] else None
-            conn.execute("""
+            cursor = conn.execute("""
                 UPDATE messages 
                 SET status = ?, processed_at = ?, error_message = ?
                 WHERE id = ?
@@ -147,7 +152,7 @@ class ReliableMessageStore:
                 message_id
             ))
             conn.commit()
-            return conn.rowcount > 0
+            return cursor.rowcount > 0
         except Exception as e:
             print(f"❌ Failed to update message status: {e}")
             return False
@@ -218,17 +223,26 @@ class HealthChecker:
     async def health_check(self) -> bool:
         """Perform health check"""
         try:
-            # Simple health check - try to check messages with reasonable timeout
-            result = await self.client.check_messages(wait=False, timeout=300, limit=1)
-            if result is not None:
+            if self.client.has_inflight_request():
+                # Another request (likely the long poll) is already in-flight.
+                # Treat this as healthy so we do not pile on redundant checks.
                 self.last_successful_check = time.time()
                 self.consecutive_failures = 0
                 return True
-            else:
-                self.consecutive_failures += 1
-                return False
+
+            # Simple health check - try to check messages with reasonable timeout
+            await self.client.check_messages(wait=False, timeout=300, limit=1)
+            self.last_successful_check = time.time()
+            self.consecutive_failures = 0
+            return True
         except Exception as e:
-            print(f"🩺 Health check failed: {e}")
+            snapshot = {}
+            try:
+                snapshot = self.client.request_snapshot()
+            except Exception:
+                pass
+            detail = f" | request={snapshot}" if snapshot else ""
+            print(f"🩺 Health check failed: {e}{detail}")
             self.consecutive_failures += 1
             return False
     
@@ -259,7 +273,13 @@ class ReliableMonitor:
         self.agent_handle_lower = ""
         self.self_mention_violation_count = 0
         self._self_handle_pattern: Optional[re.Pattern[str]] = None
-        
+        self.tool_manager: Optional[MCPToolManager] = None
+        self.plugin_config: Dict[str, Any] = {}
+        self.allowed_directories: List[str] = []
+        self.last_wait_success = time.time()
+        self.consecutive_idle_polls = 0
+        self._last_idle_report = 0.0
+
         # Load plugin configuration
         self.plugin_type = os.getenv('PLUGIN_TYPE', 'echo')
         self.plugin = None
@@ -272,19 +292,51 @@ class ReliableMonitor:
     async def initialize(self):
         """Initialize all components"""
         print("🔧 Initializing reliable monitor...")
+
+        # Ensure MCP client patches are applied before any sessions start
+        patch_mcp_library()
         
-        # Load configuration and create client
-        if self.config_path and os.path.exists(self.config_path):
-            cfg = parse_mcp_config(self.config_path)
-            self.client = MCPClient(
-                server_url=cfg.server_url,
-                oauth_server=cfg.oauth_url,
-                agent_name=cfg.agent_name,
-                token_dir=cfg.token_dir,
-            )
-        else:
+        # Load configuration and create client/tool manager
+        if not (self.config_path and os.path.exists(self.config_path)):
             raise Exception("No valid configuration found")
-        
+
+        all_servers = parse_all_mcp_servers(self.config_path)
+        if not all_servers:
+            raise Exception("No MCP servers defined in configuration")
+
+        primary_name = next(iter(all_servers.keys()))
+        primary_cfg = all_servers[primary_name]
+
+        self.client = MCPClient(
+            server_url=primary_cfg.server_url,
+            oauth_server=primary_cfg.oauth_url,
+            agent_name=primary_cfg.agent_name,
+            token_dir=primary_cfg.token_dir,
+        )
+
+        # Only create tool manager if we have additional servers to talk to
+        if len(all_servers) > 1:
+            try:
+                self.tool_manager = MCPToolManager(all_servers, primary_server=primary_name)
+            except Exception as exc:
+                print(f"⚠️ Failed to initialize tool manager: {exc}")
+                self.tool_manager = None
+        else:
+            self.tool_manager = None
+
+        # Capture filesystem roots for plugin guidance
+        allowed_dirs: set[str] = set()
+        for cfg in all_servers.values():
+            raw = getattr(cfg, "raw_config", {}) or {}
+            command = str(raw.get("command", ""))
+            args = raw.get("args") or []
+            if "server-filesystem" in command or any("server-filesystem" in str(arg) for arg in args):
+                for arg in reversed(args):
+                    if isinstance(arg, str) and arg.startswith("/"):
+                        allowed_dirs.add(arg)
+                        break
+        self.allowed_directories = sorted(allowed_dirs)
+
         # Connect with retries
         await self._reliable_connect()
         
@@ -300,10 +352,35 @@ class ReliableMonitor:
         
         # Initialize health checker
         self.health_checker = HealthChecker(self.client)
-        
+
+        # Load optional plugin config file
+        plugin_config_path = os.getenv('PLUGIN_CONFIG')
+        if plugin_config_path and os.path.exists(plugin_config_path):
+            try:
+                with open(plugin_config_path, 'r', encoding='utf-8') as fh:
+                    self.plugin_config = json.load(fh)
+            except Exception as exc:
+                print(f"⚠️ Failed to load plugin config '{plugin_config_path}': {exc}")
+                self.plugin_config = {}
+        else:
+            self.plugin_config = {}
+
         # Load plugin
         self.plugin = self._load_plugin()
-        
+
+        current_date = datetime.now(timezone.utc).strftime("%Y-%m-%d")
+        self.plugin.attach_monitor_context({
+            "current_date": current_date,
+            "tool_manager": self.tool_manager,
+            "allowed_directories": self.allowed_directories,
+        })
+        if self.tool_manager:
+            self.plugin.set_tool_manager(self.tool_manager)
+
+        now = time.time()
+        self.last_wait_success = now
+        self._last_idle_report = now
+
         print(f"✅ Reliable monitor initialized for {self.agent_handle}")
     
     async def _reliable_connect(self, max_retries: int = 10):
@@ -330,7 +407,7 @@ class ReliableMonitor:
             module = importlib.import_module(module_name)
             class_name = ''.join(word.capitalize() for word in self.plugin_type.split('_')) + 'Plugin'
             plugin_class = getattr(module, class_name)
-            return plugin_class({})
+            return plugin_class(self.plugin_config)
         except Exception as e:
             print(f"❌ Failed to load plugin '{self.plugin_type}': {e}")
             raise
@@ -358,12 +435,18 @@ class ReliableMonitor:
                 
                 # Check for new messages
                 await self._check_new_messages()
+
+                # Detect prolonged inactivity and surface diagnostics / recovery
+                await self._maybe_recover_from_idle()
                 
                 # Brief pause before next iteration
                 await asyncio.sleep(1)
                 
         except KeyboardInterrupt:
             print("\\n👋 Shutting down gracefully...")
+        except Exception as exc:
+            print(f"🛑 Monitor loop crashed unexpectedly: {exc}")
+            raise
         finally:
             # Cancel background tasks
             health_task.cancel()
@@ -379,6 +462,7 @@ class ReliableMonitor:
                     await self.client.disconnect()
                 except:
                     pass
+            print("🛑 Monitor loop terminated; exiting run().")
     
     async def _check_new_messages(self):
         """Check for new messages with reliability"""
@@ -386,13 +470,18 @@ class ReliableMonitor:
             # Use reasonable timeout for message checking
             messages = await self.client.check_messages(wait=True, timeout=300, limit=5)
             
-            if messages and '✅ WAIT SUCCESS' in messages:
+            if messages:
+                # Log received messages for debugging (like simple_working_monitor.py)
+                if '✅ WAIT SUCCESS' in messages:
+                    print("\n📨 Messages received:")
+                    print(messages[:500] + "..." if len(messages) > 500 else messages)
+                
                 message_id = self._generate_message_id(messages)
                 
                 # Check if we've already processed this message
                 if self._is_duplicate_message(message_id):
                     return
-                
+
                 # Store raw message immediately for persistence
                 stored_message = StoredMessage(
                     id=message_id,
@@ -411,7 +500,31 @@ class ReliableMonitor:
                     print(f"📥 New message stored: {message_id[:8]}...")
                 else:
                     print(f"❌ Failed to store message: {message_id[:8]}...")
-                    
+                self.last_wait_success = time.time()
+                self.consecutive_idle_polls = 0
+            else:
+                # No message returned (likely due to retry exhaustion); track idle state
+                self.consecutive_idle_polls += 1
+                if self.consecutive_idle_polls in {30, 60, 120}:
+                    snapshot = self.client.request_snapshot() if self.client else {}
+                    pending = self._count_pending_messages()
+                    waited = int(time.time() - self.last_wait_success)
+                    current = snapshot.get('label') if snapshot else 'n/a'
+                    inflight = snapshot.get('inflight') if snapshot else 'n/a'
+                    elapsed_val = snapshot.get('elapsed') if snapshot else None
+                    elapsed_str = (
+                        f"{float(elapsed_val):.3f}"
+                        if isinstance(elapsed_val, (int, float))
+                        else "n/a"
+                    )
+                    print(
+                        "⏳ Still waiting for mentions: "
+                        f"idle={waited}s, inflight={inflight}, "
+                        f"current={current}, "
+                        f"elapsed={elapsed_str}s, "
+                        f"pending={pending}"
+                    )
+
         except asyncio.TimeoutError:
             # Timeout is expected in polling mode
             pass
@@ -548,7 +661,7 @@ class ReliableMonitor:
             base = parts[0] if parts else ""
             base = base.strip('@,:')
             base = re.sub(r'[^0-9A-Za-z_\\-]', '', base)
-            if base:
+            if base and not base.lower().startswith('✅'.lower()) and 'waitsuccess' not in base.lower():
                 return f"@{base}"
         
         return "@unknown"
@@ -559,8 +672,9 @@ class ReliableMonitor:
     
     async def _process_with_plugin(self, message: StoredMessage) -> str:
         """Process message with plugin"""
+        sender = self._resolve_sender_handle(message)
         plugin_context = {
-            "sender": message.sender_handle,
+            "sender": sender,
             "agent_name": self.agent_handle,
             "session_id": None,
         }
@@ -568,7 +682,7 @@ class ReliableMonitor:
         enhanced_message = (
             "aX Platform Message Received\\n"
             f"- Your agent handle: {self.agent_handle}\\n"
-            f"- Mention originated from: {message.sender_handle}\\n"
+            f"- Mention originated from: {sender}\\n"
             "- The sender tagged you in a shared conversation.\\n\\n"
             "MESSAGE CONTENT:\\n"
             f"{message.parsed_mention}"
@@ -597,6 +711,24 @@ class ReliableMonitor:
             return sanitized_response.rstrip() + "\n" + penalty_note
 
         return response
+
+    def _resolve_sender_handle(self, message: StoredMessage) -> str:
+        sender = message.sender_handle or "@unknown"
+        if sender and sender != "@unknown":
+            return sender
+
+        text = message.parsed_mention or ""
+        mentions = re.findall(r"@[0-9A-Za-z_\-]+", text)
+        for mention in mentions:
+            if mention.lower() != self.agent_handle_lower:
+                return mention
+
+        if message.parsed_author:
+            author = message.parsed_author.strip()
+            if author and not author.lower().startswith('✅'.lower()):
+                return author
+
+        return sender
     
     async def _send_response_reliably(self, response: str, max_attempts: int = 3) -> bool:
         """Send response with retry logic"""
@@ -617,9 +749,12 @@ class ReliableMonitor:
             await self.client.disconnect()
         except:
             pass
-        
+
+        reconnect_start = time.time()
+        print("🔌 Initiating reconnect to MCP server...")
         await self._reliable_connect()
-        
+        print(f"🔗 Reconnect complete in {(time.time() - reconnect_start)*1000:.0f} ms")
+
         # Update health checker
         self.health_checker.last_successful_check = time.time()
         self.health_checker.consecutive_failures = 0
@@ -636,7 +771,55 @@ class ReliableMonitor:
             return cursor.fetchone() is not None
         finally:
             conn.close()
-    
+
+    def _count_pending_messages(self) -> int:
+        conn = sqlite3.connect(self.message_store.db_path)
+        try:
+            cursor = conn.execute(
+                "SELECT COUNT(*) FROM messages WHERE status IN ('pending', 'failed')"
+            )
+            row = cursor.fetchone()
+            return int(row[0]) if row else 0
+        finally:
+            conn.close()
+
+    async def _maybe_recover_from_idle(self) -> None:
+        if not self.client:
+            return
+
+        now = time.time()
+        idle_seconds = now - self.last_wait_success
+        if idle_seconds < 90:
+            return
+
+        if (now - self._last_idle_report) > 30:
+            snapshot = self.client.request_snapshot()
+            pending = self._count_pending_messages()
+            current = snapshot.get("label") if snapshot else "n/a"
+            elapsed_val = snapshot.get("elapsed") if snapshot else None
+            elapsed = (
+                f"{float(elapsed_val):.3f}"
+                if isinstance(elapsed_val, (int, float))
+                else "n/a"
+            )
+            inflight = snapshot.get("inflight") if snapshot else False
+            last_heartbeat = snapshot.get("last_heartbeat") if snapshot else 0.0
+            hb_age = int(now - last_heartbeat) if last_heartbeat else "n/a"
+            print(
+                "⌛ No mentions processed for "
+                f"{int(idle_seconds)}s (request={current}, inflight={inflight}, "
+                f"elapsed={elapsed}s, pending={pending}, heartbeat_age={hb_age}s)."
+            )
+            self._last_idle_report = now
+
+        if idle_seconds >= max(120, self.message_timeout // 2):
+            print(
+                "🔄 Idle threshold exceeded; recycling connection to recover long poll."
+            )
+            await self._reliable_reconnect()
+            self.last_wait_success = time.time()
+            self.consecutive_idle_polls = 0
+
     async def _retry_failed_messages(self):
         """Background task to retry failed messages"""
         while True:
@@ -686,18 +869,19 @@ class ReliableMonitor:
             
             conn = sqlite3.connect(self.message_store.db_path)
             try:
-                conn.execute("""
+                conn.execute(
+                    """
                     DELETE FROM messages 
                     WHERE status IN ('completed') 
                     AND processed_at < ?
-                """, (cutoff.isoformat(),))
-                
-                deleted = conn.rowcount
+                    """,
+                    (cutoff.isoformat(),),
+                )
+                deleted = conn.execute("SELECT changes()").fetchone()[0]
                 conn.commit()
-                
-                if deleted > 0:
+                if deleted:
                     print(f"🧹 Cleaned up {deleted} old messages")
-                    
+                
             finally:
                 conn.close()
 
diff --git a/scripts/run_langgraph_monitor.sh b/scripts/run_langgraph_monitor.sh
index 9b40b80..0a3230a 100755
--- a/scripts/run_langgraph_monitor.sh
+++ b/scripts/run_langgraph_monitor.sh
@@ -49,4 +49,4 @@ export BASE_SYSTEM_PROMPT_PATH="$PROMPT_PATH"
 export OPENROUTER_BASE_PROMPT_FILE="$PROMPT_PATH"
 export LANGGRAPH_BASE_PROMPT_FILE="$PROMPT_PATH"
 
-exec uv run python simple_working_monitor.py --loop "$@"
+exec uv run reliable_monitor.py --loop "$@"
diff --git a/scripts/start_universal_monitor.sh b/scripts/start_universal_monitor.sh
index e250c14..b68b369 100755
--- a/scripts/start_universal_monitor.sh
+++ b/scripts/start_universal_monitor.sh
@@ -955,7 +955,7 @@ run_ai_battle_mode() {
     echo
 
     echo "📡 Player 2 starting up..."
-    uv run python simple_working_monitor.py --loop &
+    uv run reliable_monitor.py --loop &
     local player2_pid=$!
 
     sleep 5
@@ -1030,9 +1030,9 @@ run_ai_battle_mode() {
         fi
 
         echo "   Cleaning up any remaining monitor processes..."
-        pkill -f "simple_working_monitor.py" 2>/dev/null || true
-        pkill -f "python.*simple_working_monitor" 2>/dev/null || true
-        pkill -f "uv run python simple_working_monitor" 2>/dev/null || true
+        pkill -f "reliable_monitor.py" 2>/dev/null || true
+        pkill -f "python.*reliable_monitor" 2>/dev/null || true
+        pkill -f "uv run reliable_monitor" 2>/dev/null || true
 
         echo -e "${GREEN}✅ All processes stopped cleanly${NC}"
         echo "👋 Battle ended!"
@@ -1063,7 +1063,7 @@ run_ai_battle_mode() {
     local quit_handler_pid=$!
 
     set +e
-    uv run python simple_working_monitor.py --loop
+    uv run reliable_monitor.py --loop
     local initiator_exit=$?
     set -e
 
@@ -2453,10 +2453,10 @@ echo ""
 monitor_exit=0
 set +e
 if (( ${#FORWARD_ARGS[@]} )); then
-    uv run python simple_working_monitor.py --loop "${FORWARD_ARGS[@]}"
+    uv run reliable_monitor.py --loop "${FORWARD_ARGS[@]}"
     monitor_exit=$?
 else
-    uv run python simple_working_monitor.py --loop
+    uv run reliable_monitor.py --loop
     monitor_exit=$?
 fi
 set -e
diff --git a/src/ax_mcp_wait_client/mcp_client.py b/src/ax_mcp_wait_client/mcp_client.py
index 5025575..c80fe04 100644
--- a/src/ax_mcp_wait_client/mcp_client.py
+++ b/src/ax_mcp_wait_client/mcp_client.py
@@ -8,6 +8,7 @@ import json
 import time
 import asyncio
 import logging
+from contextlib import asynccontextmanager
 from pathlib import Path
 from typing import Optional, Dict, Any
 from datetime import datetime, timedelta, timezone
@@ -169,12 +170,16 @@ class MCPClient:
         agent_name: str = "mcp_client_local",
         token_dir: Optional[str] = None,
         token_refresh_seconds: int = 600,
+        heartbeat_interval: Optional[int] = None,
+        heartbeat_timeout: Optional[int] = None,
     ) -> None:
         self.server_url = server_url
         self.agent_name = agent_name
         self._lock = asyncio.Lock()
         self._connected = False
         self._start_ts = time.time()
+        self._heartbeat_task: Optional[asyncio.Task[None]] = None
+        self._last_heartbeat = 0.0
 
         if not token_dir:
             token_dir = os.getenv("MCP_REMOTE_CONFIG_DIR")
@@ -197,6 +202,36 @@ class MCPClient:
         self.session: Optional[ClientSession] = None
         self.client_instance = None
         self.session_id = None
+        
+        # Serialize access to the messages tool so long polls and heartbeats
+        # never collide on the same transport.
+        self._request_lock = asyncio.Lock()
+        self._current_request_label: Optional[str] = None
+        self._current_request_started: float = 0.0
+        self._last_request_completed: float = 0.0
+        self._long_poll_active = False
+        # Default long-poll guard to 20 minutes; override via MCP_LONG_POLL_TIMEOUT env.
+        self.long_poll_timeout = int(os.getenv("MCP_LONG_POLL_TIMEOUT", "1200"))
+        self._disconnected_since: Optional[float] = None
+
+        # Heartbeat behaviour (env overrides win when explicit values are None)
+        interval_env = os.getenv("MCP_HEARTBEAT_INTERVAL", "45")
+        timeout_env = os.getenv("MCP_HEARTBEAT_TIMEOUT", "15")
+        try:
+            default_interval = int(interval_env)
+        except ValueError:
+            default_interval = 45
+        try:
+            default_timeout = int(timeout_env)
+        except ValueError:
+            default_timeout = 15
+
+        self.heartbeat_interval = (
+            heartbeat_interval if heartbeat_interval is not None else default_interval
+        )
+        self.heartbeat_timeout = (
+            heartbeat_timeout if heartbeat_timeout is not None else default_timeout
+        )
 
     async def connect(self) -> bool:
         async with self._lock:
@@ -256,14 +291,26 @@ class MCPClient:
                     except Exception:
                         pass
                 self._connected = True
+                self._start_heartbeat()
+                if self._disconnected_since:
+                    downtime = max(0.0, time.time() - self._disconnected_since)
+                    print(f"🔗 Reconnected to MCP server in {downtime*1000:.0f} ms")
+                    self._disconnected_since = None
                 return True
             except Exception as e:
+                status = None
+                if isinstance(e, httpx.HTTPStatusError) and e.response is not None:
+                    status = e.response.status_code
                 logger.error(f"Connection failed: {e}")
+                if status == 401:
+                    logger.warning("401 during connect; attempting token refresh")
+                    self.token_manager.refresh_token(force=True)
                 await self.disconnect()
                 return False
 
     async def disconnect(self) -> None:
         async with self._lock:
+            await self._stop_heartbeat()
             if self.session:
                 try:
                     await self.session.__aexit__(None, None, None)
@@ -278,8 +325,134 @@ class MCPClient:
                 self._stream_ctx = None
             self.read = self.write = self.get_sid = None
             self._connected = False
+            self._last_heartbeat = 0.0
+            self._disconnected_since = time.time()
+
+    @asynccontextmanager
+    async def _request_guard(self, label: str, *, long_poll: bool = False):
+        """Serialize message tool calls and capture request telemetry."""
+
+        await self._request_lock.acquire()
+        self._current_request_label = label
+        self._current_request_started = time.time()
+        if long_poll:
+            self._long_poll_active = True
+        try:
+            yield
+        finally:
+            if long_poll:
+                self._long_poll_active = False
+            self._last_request_completed = time.time()
+            self._current_request_label = None
+            self._request_lock.release()
+
+    def has_inflight_request(self) -> bool:
+        """Return True when any messages call is currently active."""
+
+        return self._request_lock.locked()
 
-    async def _preflight(self) -> None:
+    def is_long_poll_active(self) -> bool:
+        return self._long_poll_active
+
+    def request_snapshot(self) -> Dict[str, Any]:
+        now = time.time()
+        label = self._current_request_label
+        started = self._current_request_started if label else None
+        elapsed = (now - started) if started else 0.0
+        return {
+            "inflight": self._request_lock.locked(),
+            "label": label,
+            "started_at": started,
+            "elapsed": round(elapsed, 3),
+            "last_completed": self._last_request_completed,
+            "long_poll": self._long_poll_active,
+            "session_id": self.session_id,
+            "last_heartbeat": self._last_heartbeat,
+        }
+
+    def _start_heartbeat(self) -> None:
+        if self.heartbeat_interval <= 0:
+            return
+        if self._heartbeat_task and not self._heartbeat_task.done():
+            return
+        self._heartbeat_task = asyncio.create_task(self._heartbeat_loop())
+
+    async def _stop_heartbeat(self) -> None:
+        task = self._heartbeat_task
+        if not task:
+            return
+        self._heartbeat_task = None
+        task.cancel()
+        if asyncio.current_task() is task:
+            # Avoid awaiting on ourselves; cancellation will unwind naturally.
+            return
+        try:
+            await task
+        except asyncio.CancelledError:
+            pass
+        except Exception as exc:
+            logger.debug("Heartbeat task closed with error: %s", exc)
+
+    async def _heartbeat_loop(self) -> None:
+        # Lightweight keep-alive that leverages MCP ping when available, otherwise
+        # falls back to a zero-wait message check to touch the transport.
+        try:
+            while True:
+                await asyncio.sleep(max(1, self.heartbeat_interval))
+
+                if not self._connected or not self.session:
+                    continue
+
+                try:
+                    start = time.time()
+                    await asyncio.wait_for(
+                        self._send_ping(), timeout=max(1, self.heartbeat_timeout)
+                    )
+                    latency_ms = (time.time() - start) * 1000
+                    if latency_ms < 1:
+                        latency_ms = 1
+                    print(f"💓 Heartbeat ok ({latency_ms:.0f} ms)")
+                    self._last_heartbeat = time.time()
+                except asyncio.CancelledError:
+                    raise
+                except Exception as exc:
+                    logger.warning("Heartbeat ping failed: %s", exc)
+                    print(f"💔 Heartbeat failed: {exc}")
+                    print("💔 Heartbeat failed – closing stream for reconnect")
+                    if isinstance(exc, httpx.HTTPStatusError) and exc.response is not None and exc.response.status_code == 401:
+                        logger.warning("401 during heartbeat; refreshing token")
+                        self.token_manager.refresh_token(force=True)
+                    # Proactively drop transport so the next operation reconnects.
+                    await self.disconnect()
+                    return
+        except asyncio.CancelledError:
+            pass
+
+    async def _send_ping(self) -> None:
+        if not self.session:
+            return
+
+        if self._request_lock.locked():
+            logger.debug("Skipping heartbeat ping; another request is in flight")
+            return
+
+        async with self._request_guard("heartbeat.ping"):
+            # Prefer the explicit ping method when the SDK provides it.
+            ping_method = getattr(self.session, "ping", None)
+            if callable(ping_method):
+                await ping_method()
+                return
+
+            # Fallback: run a zero-wait message check to keep the stream warm.
+            payload = {
+                "action": "check",
+                "wait": False,
+                "mode": "latest",
+                "limit": 0,
+            }
+            await self.session.call_tool("messages", payload)
+
+    async def _preflight_locked(self) -> None:
         if not self.session:
             return
         try:
@@ -309,33 +482,102 @@ class MCPClient:
                 backoff = min(backoff * 2, 10)
                 continue
             try:
-                _stream_logger = logging.getLogger('mcp.client.streamable_http')
-                _prev_level = _stream_logger.level
-                if (time.time() - self._start_ts) < 10 and attempt == 0:
-                    _stream_logger.setLevel(logging.CRITICAL)
-                res = await self.session.call_tool(
-                    "messages",
-                    {
-                        "action": "check",
-                        "wait": wait,
-                        "wait_mode": "mentions" if wait else None,
-                        "timeout": timeout if wait else None,
-                        "mode": "latest",
-                        "limit": limit,
-                    },
-                )
+                res = None
+                request_id = None
+                async with self._request_guard("messages.check", long_poll=wait):
+                    _stream_logger = logging.getLogger('mcp.client.streamable_http')
+                    _prev_level = _stream_logger.level
+                    if (time.time() - self._start_ts) < 10 and attempt == 0:
+                        _stream_logger.setLevel(logging.CRITICAL)
+                    try:
+                        request_id = getattr(self.session, "_request_id", None)
+                        call_started = time.time()
+                        effective_timeout = timeout
+                        if wait:
+                            effective_timeout = min(timeout or self.long_poll_timeout, self.long_poll_timeout)
+                        call_kwargs = {
+                            "action": "check",
+                            "wait": wait,
+                            "wait_mode": "mentions" if wait else None,
+                            "timeout": effective_timeout if wait else None,
+                            "mode": "latest",
+                            "limit": limit,
+                        }
+
+                        async def _do_call():
+                            return await self.session.call_tool("messages", call_kwargs)
+
+                        if wait:
+                            guard_timeout = self.long_poll_timeout + 5
+                            res = await asyncio.wait_for(
+                                _do_call(),
+                                timeout=guard_timeout,
+                            )
+                        else:
+                            res = await _do_call()
+                        call_duration = time.time() - call_started
+                        if wait:
+                            print(
+                                f"📡 Long poll completed in {call_duration:.1f}s"
+                            )
+                    finally:
+                        try:
+                            _stream_logger.setLevel(_prev_level)
+                        except Exception:
+                            pass
+
                 text = None
-                for c in getattr(res, "content", []) or []:
-                    if getattr(c, "type", "") == "text" and hasattr(c, "text"):
-                        text = c.text
-                        break
-                return text or str(getattr(res, "__dict__", res))
+                if res is not None:
+                    for c in getattr(res, "content", []) or []:
+                        if getattr(c, "type", "") == "text" and hasattr(c, "text"):
+                            text = c.text
+                            break
+                    if text:
+                        return text
+                    return str(getattr(res, "__dict__", res))
+                return None
+            except httpx.HTTPStatusError as exc:
+                status = exc.response.status_code if exc.response else None
+                logger.warning(
+                    "HTTP %s while checking messages: %s", status, exc
+                )
+                if status == 401:
+                    logger.warning("401 while checking messages; refreshing token")
+                    self.token_manager.refresh_token(force=True)
+                    await asyncio.shield(self.disconnect())
+                    await asyncio.sleep(backoff)
+                    backoff = min(backoff * 2, 10)
+                    continue
+                if status is not None and status >= 500:
+                    await asyncio.shield(self.disconnect())
+                await asyncio.sleep(backoff)
+                backoff = min(backoff * 2, 10)
+                continue
+            except asyncio.CancelledError as exc:
+                logger.warning("Check messages cancelled (transport reset): %s", exc)
+                await self.disconnect()
+                await asyncio.sleep(backoff)
+                backoff = min(backoff * 2, 10)
+                continue
+            except asyncio.TimeoutError as exc:
+                logger.warning("Check messages timed out awaiting response: %s", exc)
+                elapsed = 0.0
+                if 'call_started' in locals():
+                    elapsed = time.time() - call_started
+                print(
+                    "⏳ Long poll guard tripped after "
+                    f"{elapsed:.1f}s (limit {self.long_poll_timeout}s); reconnecting"
+                )
+                await self._cancel_request(request_id, "timeout")
+                await asyncio.sleep(backoff)
+                backoff = min(backoff * 2, 10)
+                continue
             except Exception as e:
                 msg = str(e)
                 if "401" in msg:
                     logger.warning("401 on check; refreshing token with backoff")
                     self.token_manager.refresh_token(force=True)
-                    await self.disconnect()
+                    await asyncio.shield(self.disconnect())
                     await asyncio.sleep(backoff)
                     backoff = min(backoff * 2, 10)
                     continue
@@ -346,11 +588,6 @@ class MCPClient:
                 await self.disconnect()
                 await asyncio.sleep(backoff)
                 backoff = min(backoff * 2, 10)
-            finally:
-                try:
-                    _stream_logger.setLevel(_prev_level)
-                except Exception:
-                    pass
         return None
 
     async def send_message(self, message: str) -> bool:
@@ -363,34 +600,75 @@ class MCPClient:
                 backoff = min(backoff * 2, 10)
                 continue
             try:
-                await self._preflight()
-                _stream_logger = logging.getLogger('mcp.client.streamable_http')
-                _prev_level = _stream_logger.level
-                if (time.time() - self._start_ts) < 10 and attempt == 0:
-                    _stream_logger.setLevel(logging.CRITICAL)
-                res = await self.session.call_tool(
-                    "messages",
-                    {"action": "send", "content": message, "idempotency_key": idem_key},
-                )
+                res = None
+                request_id = None
+                async with self._request_guard("messages.send"):
+                    await self._preflight_locked()
+                    _stream_logger = logging.getLogger('mcp.client.streamable_http')
+                    _prev_level = _stream_logger.level
+                    if (time.time() - self._start_ts) < 10 and attempt == 0:
+                        _stream_logger.setLevel(logging.CRITICAL)
+                    try:
+                        request_id = getattr(self.session, "_request_id", None)
+                        res = await self.session.call_tool(
+                            "messages",
+                            {"action": "send", "content": message, "idempotency_key": idem_key},
+                        )
+                    finally:
+                        try:
+                            _stream_logger.setLevel(_prev_level)
+                        except Exception:
+                            pass
+
                 # Consider any response a success; server-side idempotency should dedupe
                 text = None
-                for c in getattr(res, "content", []) or []:
-                    if getattr(c, "type", "") == "text" and hasattr(c, "text"):
-                        text = c.text
-                        break
+                if res is not None:
+                    for c in getattr(res, "content", []) or []:
+                        if getattr(c, "type", "") == "text" and hasattr(c, "text"):
+                            text = c.text
+                            break
                 logger.info(f"message sent (idem={idem_key}) -> {text or 'ok'}")
                 return True
+            except httpx.HTTPStatusError as exc:
+                status = exc.response.status_code if exc.response else None
+                logger.warning(
+                    "HTTP %s while sending message: %s", status, exc
+                )
+                if status == 401:
+                    logger.warning("401 while sending message; refreshing token")
+                    self.token_manager.refresh_token(force=True)
+                    await asyncio.shield(self.disconnect())
+                    await asyncio.sleep(backoff)
+                    backoff = min(backoff * 2, 10)
+                    continue
+                if status is not None and status >= 500:
+                    await asyncio.shield(self.disconnect())
+                await asyncio.sleep(backoff)
+                backoff = min(backoff * 2, 10)
+                continue
+            except asyncio.CancelledError as exc:
+                logger.warning("Send message cancelled (transport reset): %s", exc)
+                await self.disconnect()
+                await asyncio.sleep(backoff)
+                backoff = min(backoff * 2, 10)
+                continue
+            except asyncio.TimeoutError as exc:
+                logger.warning("Send message timed out awaiting result: %s", exc)
+                await self._cancel_request(request_id, "timeout")
+                await asyncio.sleep(backoff)
+                backoff = min(backoff * 2, 10)
+                continue
             except Exception as e:
                 msg = str(e)
                 if "401" in msg:
                     logger.warning("401 on send; refreshing token with backoff")
                     self.token_manager.refresh_token(force=True)
-                    await self.disconnect()
+                    await asyncio.shield(self.disconnect())
                     await asyncio.sleep(backoff)
                     backoff = min(backoff * 2, 10)
                     continue
                 logger.error(f"Send message failed: {e}")
-                await self.disconnect()
+                await asyncio.shield(self.disconnect())
                 await asyncio.sleep(backoff)
                 backoff = min(backoff * 2, 10)
             finally:
@@ -400,12 +678,37 @@ class MCPClient:
                     pass
         return False
 
+    async def _cancel_request(self, request_id: Any, reason: str) -> None:
+        """Issue a cancellation notification for an in-flight request."""
+        if not self.session or request_id is None:
+            return
+
+        async def _send_cancel() -> None:
+            try:
+                from mcp import types
+
+                cancel_id = request_id if isinstance(request_id, int) else request_id
+                notification = types.CancelledNotification(
+                    params=types.CancelledNotificationParams(requestId=cancel_id, reason=reason)
+                )
+                await self.session.send_notification(notification)
+            except asyncio.CancelledError:
+                logger.debug("Cancellation notification aborted: session closing")
+            except Exception as exc:
+                logger.debug("Failed to send cancellation notification: %s", exc)
+
+        try:
+            asyncio.create_task(_send_cancel())
+        except RuntimeError:
+            # Event loop may be shutting down; best-effort only
+            pass
+
 
-async def simple_example() -> None:
-    logging.basicConfig(level=logging.INFO)
-    client = MCPClient(token_refresh_seconds=600)
-    ok = await client.send_message("[MCPClient] Test message from persistent client")
-    print(f"Result: {ok}")
+    async def simple_example() -> None:
+        logging.basicConfig(level=logging.INFO)
+        client = MCPClient(token_refresh_seconds=600)
+        ok = await client.send_message("[MCPClient] Test message from persistent client")
+        print(f"Result: {ok}")
 
 
 if __name__ == "__main__":
diff --git a/src/ax_mcp_wait_client/mcp_patches.py b/src/ax_mcp_wait_client/mcp_patches.py
index 61284d8..b5f5901 100644
--- a/src/ax_mcp_wait_client/mcp_patches.py
+++ b/src/ax_mcp_wait_client/mcp_patches.py
@@ -1,68 +1,146 @@
 """Monkey patches for MCP library to handle non-compliant server responses."""
 
+from __future__ import annotations
+
 import json
 import logging
-from typing import Any, Optional
-from mcp.types import JSONRPCMessage, JSONRPCError
+import time
+from typing import Any, Awaitable, Callable
+
+import httpx
+
+from mcp.client.streamable_http import StreamableHTTPTransport
+from mcp.types import JSONRPCMessage
 
 logger = logging.getLogger(__name__)
 
-# Store the original method
-_original_validate_json = None
+_original_validate_json: Callable[..., JSONRPCMessage] | None = None
+_original_handle_sse_event: Callable[..., Awaitable[bool]] | None = None
+_original_handle_post_request: Callable[..., Awaitable[None]] | None = None
+
 
-def patch_mcp_library():
+def patch_mcp_library() -> None:
     """Apply monkey patches to MCP library to handle server quirks."""
-    global _original_validate_json
-    
-    # Only patch once
-    if _original_validate_json is not None:
-        return
-    
-    # Save the original method
-    _original_validate_json = JSONRPCMessage.model_validate_json
-    
-    @classmethod
-    def patched_model_validate_json(cls, json_data: str | bytes, **kwargs):
-        """Patched version that fixes null IDs in error responses."""
-        try:
-            # Try to parse and fix the JSON
-            data = json.loads(json_data)
-            
-            # Fix server bug: error responses with id:null
-            if isinstance(data, dict) and "error" in data and data.get("id") is None:
-                # Use a placeholder ID that won't match any real request
-                data["id"] = "error-null-id-fixed"
-                logger.debug(f"Fixed null id in error response: {data.get('error', {}).get('message', 'unknown')}")
-                json_data = json.dumps(data)
-            
-            # Call the original method with the fixed data
-            return _original_validate_json(json_data, **kwargs)
-        except json.JSONDecodeError:
-            # If it's not valid JSON, let the original method handle it
-            return _original_validate_json(json_data, **kwargs)
-        except Exception as e:
-            # For any other error, try the original method
-            logger.debug(f"Error in patched validation, falling back: {e}")
-            return _original_validate_json(json_data, **kwargs)
-    
-    # Apply the monkey patch
-    JSONRPCMessage.model_validate_json = patched_model_validate_json
-    logger.info("Applied MCP library patches for server compatibility")
-
-    # NOTE: We intentionally do NOT override OAuthClientProvider.async_auth_flow here
-    # because the default implementation performs standards-compliant discovery.
-    # Duplicates should be handled by server-side auth-before-side-effect or
-    # idempotency, not by bypassing discovery.
-
-
-def unpatch_mcp_library():
+
+    global _original_validate_json, _original_handle_sse_event, _original_handle_post_request
+
+    if _original_validate_json is None:
+        _original_validate_json = JSONRPCMessage.model_validate_json
+
+        @classmethod
+        def patched_model_validate_json(cls, json_data: str | bytes, **kwargs: Any) -> JSONRPCMessage:
+            # Skip completely empty keep-alive payloads
+            if isinstance(json_data, bytes):
+                text = json_data.decode("utf-8", "ignore")
+            else:
+                text = str(json_data)
+
+            if not text.strip():
+                raise ValueError("Empty SSE payload")
+
+            try:
+                data = json.loads(text)
+                if isinstance(data, dict) and "error" in data and data.get("id") is None:
+                    data["id"] = "error-null-id-fixed"
+                    logger.debug(
+                        "Fixed null id in error response: %s",
+                        data.get("error", {}).get("message", "unknown"),
+                    )
+                    text = json.dumps(data)
+            except ValueError:
+                # json.loads failed; fall through to original handler
+                pass
+
+            return _original_validate_json(text, **kwargs)  # type: ignore[arg-type]
+
+        JSONRPCMessage.model_validate_json = patched_model_validate_json  # type: ignore[assignment]
+        logger.info("Applied JSON validation patch for MCP stream handling")
+
+    if _original_handle_sse_event is None:
+        _original_handle_sse_event = StreamableHTTPTransport._handle_sse_event
+
+        async def patched_handle_sse_event(
+            self,
+            sse,
+            read_stream_writer,
+            original_request_id=None,
+            resumption_callback=None,
+            is_initialization: bool = False,
+        ) -> bool:
+            data = getattr(sse, "data", "")
+            if sse.event == "message" and (data is None or not str(data).strip()):
+                logger.debug("Skipping empty SSE frame from streamable HTTP")
+                return False
+
+            try:
+                return await _original_handle_sse_event(
+                    self,
+                    sse,
+                    read_stream_writer,
+                    original_request_id,
+                    resumption_callback,
+                    is_initialization,
+                )
+            except ValueError as exc:
+                # Raised when our patched JSON validator encounters an empty payload
+                logger.debug("Ignored SSE frame due to empty payload: %s", exc)
+                return False
+
+        StreamableHTTPTransport._handle_sse_event = patched_handle_sse_event  # type: ignore[assignment]
+        logger.info("Applied SSE handler patch for MCP stream handling")
+
+    if _original_handle_post_request is None:
+        _original_handle_post_request = StreamableHTTPTransport._handle_post_request
+
+        async def patched_handle_post_request(self, ctx):
+            self._connection_start_ts = time.time()
+            try:
+                await _original_handle_post_request(self, ctx)
+            except httpx.HTTPStatusError as exc:
+                status = exc.response.status_code if exc.response else None
+                if status == httpx.codes.GATEWAY_TIMEOUT:
+                    elapsed = time.time() - getattr(self, "_connection_start_ts", time.time())
+                    logger.info(
+                        "StreamableHTTP long poll timed out after %.1fs (HTTP %s). Reissuing wait.",
+                        elapsed,
+                        status,
+                    )
+                    try:
+                        await ctx.read_stream_writer.send(exc)
+                    except Exception:
+                        logger.debug("Failed to forward HTTP 504 to read stream")
+                else:
+                    logger.warning("StreamableHTTP POST failed with %s: %s", status, exc)
+                    try:
+                        await ctx.read_stream_writer.send(exc)
+                    except Exception:
+                        logger.debug("Failed to forward HTTP error to read stream")
+
+        StreamableHTTPTransport._handle_post_request = patched_handle_post_request  # type: ignore[assignment]
+        logger.info("Applied POST handler patch for MCP stream handling")
+
+
+def unpatch_mcp_library() -> None:
     """Remove monkey patches from MCP library."""
-    global _original_validate_json
-    
+
+    global _original_validate_json, _original_handle_sse_event
+
     if _original_validate_json is not None:
-        JSONRPCMessage.model_validate_json = _original_validate_json
+        JSONRPCMessage.model_validate_json = _original_validate_json  # type: ignore[assignment]
         _original_validate_json = None
-        logger.info("Removed MCP library patches")
+        logger.info("Removed JSON validation patch")
+
+    if _original_handle_sse_event is not None:
+        StreamableHTTPTransport._handle_sse_event = _original_handle_sse_event  # type: ignore[assignment]
+        _original_handle_sse_event = None
+        logger.info("Removed SSE handler patch")
+
+    global _original_handle_post_request
+    if _original_handle_post_request is not None:
+        StreamableHTTPTransport._handle_post_request = _original_handle_post_request  # type: ignore[assignment]
+        _original_handle_post_request = None
+        logger.info("Removed POST handler patch")
+
 
-# Alias for compatibility
+# Alias for backwards compatibility
 apply_patches = patch_mcp_library
#!/usr/bin/env python3
"""Heartbeat-based MCP monitor with optional LangGraph responses.

This script keeps a reliable long-poll connection alive using the ``mcp-use``
client (heartbeat pings, stall detection, automatic reconnection) and now
supports routing each mention through a LangGraph workflow powered by the
LangChain MCP adapter.  When the LangGraph plugin is enabled the monitor can
call remote MCP tools (e.g., filesystem, web search) while answering with
OpenRouter ``x-ai/grok-4-fast:free`` or other configured models.
"""

from __future__ import annotations

import argparse
import asyncio
import hashlib
import importlib
import json
import logging
import os
import sys
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional, Tuple

from mcp_use import MCPClient

# Ensure project root is importable when running from scripts/
REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from ax_mcp_wait_client.config_loader import parse_all_mcp_servers
from mcp_tool_manager import MCPToolManager
from plugins.base_plugin import BasePlugin

logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')
for noisy_logger, level in (
    ('httpx', logging.WARNING),
    ('mcp', logging.CRITICAL),
    ('mcp.client', logging.CRITICAL),
    ('mcp.client.streamable_http', logging.CRITICAL),
    ('pydantic', logging.CRITICAL),
    ('pydantic_core', logging.CRITICAL),
):
    logging.getLogger(noisy_logger).setLevel(level)

DEFAULT_WAIT_TIMEOUT = 35  # seconds — below the proxy timeout
HEARTBEAT_INTERVAL = 25
HEARTBEAT_TIMEOUT = 10
HEARTBEAT_SUMMARY_INTERVAL = 72  # ~30 minutes at 25s interval
MAX_BACKOFF_RETRIES = 3
STALL_THRESHOLD = 120  # seconds without mention or heartbeat => reconnect


def _load_server_name(client: MCPClient) -> str:
    servers = list(client.config.get("mcpServers", {}).keys())
    if not servers:
        raise RuntimeError("No servers defined in MCP configuration")
    return servers[0]


def _resolve_agent_handle(server_cfg: dict) -> Optional[str]:
    name = (server_cfg.get("agent") or {}).get("name")
    if name:
        return name
    env_block = server_cfg.get("env") or {}
    if "X-Agent-Name" in env_block:
        return env_block["X-Agent-Name"]
    args = server_cfg.get("args") or []
    for idx, arg in enumerate(args):
        if str(arg).lower() == "--header" and idx + 1 < len(args):
            header = str(args[idx + 1])
            if header.lower().startswith("x-agent-name:"):
                return header.split(":", 1)[1].strip()
    return None


def _extract_sender(raw: str, self_handle: str) -> Tuple[str, str]:
    lines = raw.replace("\\n", "\n").splitlines()
    for line in lines:
        line = line.strip()
        if not line:
            continue
        if line.startswith("•") or line.startswith("-"):
            if ":" not in line:
                continue
            author, body = line.split(":", 1)
            author = author.lstrip("•- \t")
            handles = [token for token in author.split() if token.startswith("@")]
            handles += [token for token in body.split() if token.startswith("@")]
            for handle in handles:
                if handle.lower() != self_handle.lower():
                    return author or "unknown", handle
            if author and not author.lower().startswith("✅ wait success"):
                base = author.split()[0].strip("@,:")
                if base and base.lower() != self_handle.lower():
                    return author, f"@{base}"
    return "unknown", "@unknown"


def _message_id(raw: str) -> str:
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()


def _build_ack(sender: str, ping_id: str) -> str:
    if not sender.startswith("@"):
        sender = f"@{sender}"
    return f"{sender} — Ack ({ping_id})"


def _extract_text(result) -> str:
    text_parts = []
    for item in getattr(result, "content", []) or []:
        text = getattr(item, "text", None)
        if text:
            text_parts.append(text)
    return "".join(text_parts)


def _load_plugin(plugin_type: str, plugin_config: Optional[dict]) -> BasePlugin:
    module_name = f"plugins.{plugin_type}_plugin"
    module = importlib.import_module(module_name)
    class_name = "".join(word.capitalize() for word in plugin_type.split("_")) + "Plugin"
    plugin_class = getattr(module, class_name)
    return plugin_class(plugin_config or {})


def _read_plugin_config(path: Optional[str]) -> dict:
    if not path:
        return {}
    file_path = Path(path).expanduser()
    if not file_path.exists():
        raise FileNotFoundError(f"Plugin config not found: {file_path}")
    with file_path.open("r", encoding="utf-8") as fh:
        return json.load(fh)


def _allowed_directories(server_configs) -> list[str]:
    roots: set[str] = set()
    for cfg in server_configs.values():
        raw = getattr(cfg, "raw_config", {}) or {}
        command = str(raw.get("command", ""))
        args = raw.get("args") or []
        if "server-filesystem" in command or any("server-filesystem" in str(arg) for arg in args):
            for arg in reversed(args):
                if isinstance(arg, str) and arg.startswith("/"):
                    roots.add(arg)
                    break
    return sorted(roots)


def _extract_message_text(raw: str) -> str:
    """Return the human-authored content from a mention block."""
    lines = [line.strip() for line in raw.splitlines() if line.strip()]
    cleaned = [line for line in lines if not line.startswith("✅ WAIT SUCCESS")]
    for line in cleaned:
        if line.startswith("•") or line.startswith("-"):
            body = line.lstrip("•- \t")
            if ":" in body:
                _, tail = body.split(":", 1)
                return tail.strip()
            return body.strip()
    return raw.strip()


async def _call_messages_with_retry(
    session,
    payload,
    retries: int = MAX_BACKOFF_RETRIES,
    base_delay: float = 1.0,
    allow_504: bool = False,
    suppress_errors: bool = False,
):
    last_error = None
    for attempt in range(retries):
        try:
            return await session.call_tool("messages", payload)
        except Exception as exc:
            message = str(exc)
            if ("504" in message or "timeout" in message.lower()) and attempt < retries - 1:
                delay = base_delay * (2 ** attempt)
                print(f"⚠️ messages call failed ({message}); retrying in {delay:.1f}s")
                await asyncio.sleep(delay)
                last_error = exc
                continue
            if "504" in message and allow_504:
                print("⏳ messages wait timed out (HTTP 504)")
                return None
            last_error = exc
            break
    if last_error is not None and not suppress_errors:
        print(f"❌ messages call failed after {retries} attempts: {last_error}")
    return None

async def monitor_loop(
    config_path: str,
    stall_threshold: int,
    plugin_type: str,
    plugin_config_path: Optional[str] = None,
    wait_timeout: int = DEFAULT_WAIT_TIMEOUT,
) -> None:
    client = MCPClient.from_config_file(config_path)
    await client.create_all_sessions()
    server_name = _load_server_name(client)
    session = client.get_session(server_name)

    raw_server_cfg = client.config.get("mcpServers", {}).get(server_name, {})
    resolved = _resolve_agent_handle(raw_server_cfg) or "agent"
    if not resolved.startswith("@"):
        resolved = f"@{resolved}"
    agent_name = resolved

    # Load MCP configs for optional tool manager / LangGraph integration
    plugin: Optional[BasePlugin] = None
    tool_manager: Optional[MCPToolManager] = None
    allowed_dirs: list[str] = []

    try:
        server_configs = parse_all_mcp_servers(config_path)
    except Exception as exc:  # noqa: BLE001
        print(f"⚠️ Failed to parse MCP config for tool loading: {exc}")
        server_configs = {}

    if server_configs:
        primary_name = next(iter(server_configs.keys()))
        if len(server_configs) > 1:
            try:
                tool_manager = MCPToolManager(server_configs, primary_server=primary_name)
            except Exception as exc:  # noqa: BLE001
                print(f"⚠️ Failed to initialize MCP tool manager: {exc}")
                tool_manager = None
        allowed_dirs = _allowed_directories(server_configs)

    plugin_enabled = plugin_type not in {"", "none", "ack", "Ack", "ACK"}
    if plugin_enabled:
        config_source = plugin_config_path or os.getenv("PLUGIN_CONFIG")
        plugin_config = {}
        if config_source:
            try:
                plugin_config = _read_plugin_config(config_source)
            except Exception as exc:  # noqa: BLE001
                print(f"⚠️ Failed to load plugin config '{config_source}': {exc}")
        try:
            plugin = _load_plugin(plugin_type, plugin_config)
        except Exception as exc:  # noqa: BLE001
            print(f"❌ Failed to load plugin '{plugin_type}': {exc}")
            plugin_enabled = False
            plugin = None

    if plugin:
        current_date = datetime.now(timezone.utc).strftime("%Y-%m-%d")
        plugin.attach_monitor_context(
            {
                "current_date": current_date,
                "allowed_directories": allowed_dirs,
            }
        )
        if tool_manager:
            plugin.set_tool_manager(tool_manager)

    seen_ids: set[str] = set()

    last_activity = time.time()
    heartbeat_success_count = 0
    heartbeat_status_active = False
    heartbeat_line_length = 0

    def _emit_heartbeat(status: str) -> None:
        nonlocal heartbeat_status_active, heartbeat_line_length
        padding = " " * max(0, heartbeat_line_length - len(status))
        sys.stdout.write(f"\r{status}{padding}")
        sys.stdout.flush()
        heartbeat_status_active = True
        heartbeat_line_length = len(status)

    def _newline_from_heartbeat() -> None:
        nonlocal heartbeat_status_active, heartbeat_line_length
        if heartbeat_status_active:
            sys.stdout.write("\r" + " " * heartbeat_line_length + "\r")
            sys.stdout.flush()
            heartbeat_status_active = False
            heartbeat_line_length = 0

    def console_print(*args, **kwargs) -> None:
        _newline_from_heartbeat()
        print(*args, **kwargs)

    console_print(f"✅ Monitor connected as {agent_name} (server: {server_name})")

    async def heartbeat_loop():
        nonlocal last_activity, heartbeat_success_count
        while True:
            await asyncio.sleep(HEARTBEAT_INTERVAL)
            try:
                payload = {"action": "check", "wait": False, "limit": 0}
                started = time.time()
                result = await asyncio.wait_for(
                    session.call_tool("messages", payload), timeout=HEARTBEAT_TIMEOUT
                )
                if result is None:
                    continue
                heartbeat_success_count += 1
                last_activity = time.time()
                latency_ms = max(int((last_activity - started) * 1000), 0)
                status = "💓 Heartbeat ok"
                _emit_heartbeat(status)
            except asyncio.TimeoutError:
                _newline_from_heartbeat()
                print("💔 Heartbeat timed out")
                heartbeat_success_count = 0
            except Exception as exc:
                _newline_from_heartbeat()
                print(f"💔 Heartbeat error: {exc}")
                heartbeat_success_count = 0

    hb_task = asyncio.create_task(heartbeat_loop())

    try:
        while True:
            if time.time() - last_activity > stall_threshold:
                console_print("⚠️ Detected stall; attempting reconnect")
                await hb_task.cancel()
                await client.close_all_sessions()
                await client.create_all_sessions()
                session = client.get_session(server_name)
                last_activity = time.time()
                hb_task = asyncio.create_task(heartbeat_loop())
                continue

            payload = {
                "action": "check",
                "wait": True,
                "wait_mode": "mentions",
                "timeout": wait_timeout,
                "limit": 5,
            }
            result = await _call_messages_with_retry(
                session,
                payload,
                retries=1,
                allow_504=True,
                suppress_errors=True,
            )
            if result is None:
                continue
            raw = _extract_text(result)
            if not raw:
                continue
            mid = _message_id(raw)
            if mid in seen_ids:
                continue
            seen_ids.add(mid)
            last_activity = time.time()

            console_print("\n📨 Incoming mention block:\n" + raw)
            author, sender = _extract_sender(raw, agent_name)
            if sender.lower() == "@unknown":
                console_print("ℹ️ No valid mention detected; skipping response")
                continue

            response_text: Optional[str] = None

            if plugin_enabled and plugin:
                message_text = _extract_message_text(raw)
                metadata = {
                    "sender": sender,
                    "agent_name": agent_name,
                    "ignore_mentions": [agent_name],
                }
                try:
                    response_text = await plugin.process_message(message_text, metadata)
                except Exception as exc:  # noqa: BLE001
                    console_print(f"⚠️ Plugin processing failed: {exc}")
                    response_text = None

            if not response_text:
                response_text = _build_ack(sender, mid[:8])
                console_print(f"💬 Acknowledging {sender} (author: {author}) with {mid[:8]}")
            else:
                pass

            sent = await _call_messages_with_retry(
                session,
                {
                    "action": "send",
                    "content": response_text,
                    "idempotency_key": mid,
                },
                retries=1,
            )
            if sent is not None:
                console_print("✅ Response dispatched")
            else:
                console_print(f"❌ Response send failed for {mid[:8]}")
    except asyncio.CancelledError:
        pass
    finally:
        _newline_from_heartbeat()
        hb_task.cancel()
        await client.close_all_sessions()


def main(argv: Optional[list[str]] = None) -> int:
    parser = argparse.ArgumentParser(description="MCP-use heartbeat monitor")
    parser.add_argument("--config", required=True)
    parser.add_argument("--stall-threshold", type=int, default=STALL_THRESHOLD)
    parser.add_argument(
        "--plugin",
        default=os.getenv("PLUGIN_TYPE", "ack"),
        help="Optional plugin name (e.g., langgraph, echo). Use 'ack' to send simple acknowledgements.",
    )
    parser.add_argument(
        "--plugin-config",
        default=None,
        help="Path to plugin configuration JSON (overrides PLUGIN_CONFIG).",
    )
    parser.add_argument(
        "--wait-timeout",
        type=int,
        default=DEFAULT_WAIT_TIMEOUT,
        help="Timeout (seconds) for messages.check long polls",
    )
    args = parser.parse_args(argv)

    try:
        asyncio.run(
            monitor_loop(
                args.config,
                args.stall_threshold,
                args.plugin,
                args.plugin_config,
                args.wait_timeout,
            )
        )
    except KeyboardInterrupt:
        print("\n👋 Monitor stopped")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
